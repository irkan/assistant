import React, { useState, useRef, useCallback, useEffect } from 'react';
import {
  GoogleGenAI,
  LiveServerMessage,
  MediaResolution,
  Modality,
  Session,
  FunctionDeclaration,
} from '@google/genai';
import { SimpleAudioRecorder } from '../utils/audioUtils';

export interface MorphTargetData {
    morphTarget: string;
    weight: string;
  }

export interface AylaModelRef {
  updateMorphTargets: (targets: MorphTargetData[]) => void;
  playGreetingAnimation: () => Promise<void>;
}

interface GeminiLiveAudioProps {
  apiKey: string;
  shouldConnect?: boolean;
  shouldDisconnect?: boolean;
  shouldStartRecording?: boolean;
  onConnectionChange?: (connected: boolean) => void;
  onRecordingStart?: () => void;
  onMuteChange?: (muted: boolean) => void;
  externalMuted?: boolean;
  onVolumeChange?: (volume: number) => void;
  onInVolumeChange?: (inVolume: number) => void;
  onLipsyncUpdate?: (text: string, duration: number) => void;
  aylaModelRef?: React.RefObject<AylaModelRef>;
}

class GeminiAudioStreamer {
  public audioQueue: Float32Array[] = [];
  public isPlaying: boolean = false;
  private sampleRate: number = 24000;
  private bufferSize: number = 7680;
  private processingBuffer: Float32Array = new Float32Array(0);
  private scheduledTime: number = 0;
  public gainNode: GainNode;
  private isStreamComplete: boolean = false;
  private checkInterval: number | null = null;
  private initialBufferTime: number = 0.1;
  private endOfQueueAudioSource: AudioBufferSourceNode | null = null;

  public onComplete = () => {};
  public onAudioStart = (startTime: number) => {};
  public onAudioProgress = (currentTime: number, isPlaying: boolean) => {};
  public onLipsyncUpdate = (text: string, duration: number) => {};

  constructor(public context: AudioContext) {
    this.gainNode = this.context.createGain();
    this.gainNode.gain.value = 1.0; // Ensure volume is at 100%
    this.gainNode.connect(this.context.destination);
    console.log('üéµ GainNode created with volume:', this.gainNode.gain.value);
  }

  addBase64Audio(base64Data: string, mimeType: string) {

    // Convert base64 to PCM16
    const binaryString = atob(base64Data);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }

    this.addPCM16(bytes);
  }

  private calculateAudioDuration(pcmData: Uint8Array): number {
    // PCM16 = 2 bytes per sample, sample rate = 24000 Hz
    const sampleCount = pcmData.length / 2;
    const duration = sampleCount / this.sampleRate;
    return duration;
  }

  addPCM16(chunk: Uint8Array) {
    // Ensure AudioContext is running
    if (this.context.state === 'suspended') {
      console.log('üéµ Resuming suspended AudioContext');
      this.context.resume();
    }
    
    const float32Array = new Float32Array(chunk.length / 2);
    const dataView = new DataView(chunk.buffer);

    for (let i = 0; i < chunk.length / 2; i++) {
      try {
        const int16 = dataView.getInt16(i * 2, true);
        float32Array[i] = int16 / 32768;
      } catch (e) {
        console.error('PCM conversion error:', e);
      }
    }

    const newBuffer = new Float32Array(
      this.processingBuffer.length + float32Array.length,
    );
    newBuffer.set(this.processingBuffer);
    newBuffer.set(float32Array, this.processingBuffer.length);
    this.processingBuffer = newBuffer;

    while (this.processingBuffer.length >= this.bufferSize) {
      const buffer = this.processingBuffer.slice(0, this.bufferSize);
      this.audioQueue.push(buffer);
      this.processingBuffer = this.processingBuffer.slice(this.bufferSize);
    }

    if (!this.isPlaying) {
      this.isPlaying = true;
      this.scheduledTime = this.context.currentTime + this.initialBufferTime;
      this.onAudioStart(this.scheduledTime);
      this.scheduleNextBuffer();
    }
  }

  private createAudioBuffer(audioData: Float32Array): AudioBuffer {
    const audioBuffer = this.context.createBuffer(
      1,
      audioData.length,
      this.sampleRate,
    );
    audioBuffer.getChannelData(0).set(audioData);
    return audioBuffer;
  }

  private scheduleNextBuffer() {
    const SCHEDULE_AHEAD_TIME = 0.2;

    while (
      this.audioQueue.length > 0 &&
      this.scheduledTime < this.context.currentTime + SCHEDULE_AHEAD_TIME
    ) {
      const audioData = this.audioQueue.shift()!;
      const audioBuffer = this.createAudioBuffer(audioData);
      const source = this.context.createBufferSource();

      if (this.audioQueue.length === 0) {
        if (this.endOfQueueAudioSource) {
          this.endOfQueueAudioSource.onended = null;
        }
        this.endOfQueueAudioSource = source;
        source.onended = () => {
          // console.log('üéµ Audio chunk ended');
          if (
            !this.audioQueue.length &&
            this.endOfQueueAudioSource === source
          ) {
            this.endOfQueueAudioSource = null;
            this.onComplete();
          }
        };
      }

      source.buffer = audioBuffer;
      source.connect(this.gainNode);

      const startTime = Math.max(this.scheduledTime, this.context.currentTime);
      source.start(startTime);

      this.scheduledTime = startTime + audioBuffer.duration;
    }

    this.onAudioProgress(this.context.currentTime, this.isPlaying);

    if (this.audioQueue.length === 0 && this.processingBuffer.length === 0) {
      if (this.isStreamComplete) {
        this.isPlaying = false;
        if (this.checkInterval) {
          clearInterval(this.checkInterval);
          this.checkInterval = null;
        }
      } else {
        if (!this.checkInterval) {
          this.checkInterval = window.setInterval(() => {
            if (
              this.audioQueue.length > 0 ||
              this.processingBuffer.length >= this.bufferSize
            ) {
              this.scheduleNextBuffer();
            }
          }, 100) as unknown as number;
        }
      }
    } else {
      const nextCheckTime =
        (this.scheduledTime - this.context.currentTime) * 1000;
      setTimeout(
        () => this.scheduleNextBuffer(),
        Math.max(0, nextCheckTime - 50),
      );
    }
  }

  stop() {
    this.isPlaying = false;
    this.isStreamComplete = true;
    this.audioQueue = [];
    this.processingBuffer = new Float32Array(0);
    this.scheduledTime = this.context.currentTime;
    
    // Stop any currently playing audio source
    if (this.endOfQueueAudioSource) {
      try {
        this.endOfQueueAudioSource.stop();
        this.endOfQueueAudioSource.disconnect();
        console.log('üõë Stopped active audio source');
      } catch (e) {
        console.log('üõë Audio source already stopped');
      }
      this.endOfQueueAudioSource = null;
    }
    
    // Clear any scheduled intervals
    if (this.checkInterval) {
      clearInterval(this.checkInterval);
      this.checkInterval = null;
    }
    
    // Reset stream complete flag for next playback
    setTimeout(() => {
      this.isStreamComplete = false;
      // Ensure gainNode is still connected
      if (this.gainNode.numberOfOutputs === 0) {
        this.gainNode.connect(this.context.destination);
      }
      // Ensure volume is correct
      this.gainNode.gain.value = 1.0;
      console.log('üîÑ Audio streamer reset for next playback, volume:', this.gainNode.gain.value);
    }, 100);

    this.onAudioProgress(this.context.currentTime, false);

    if (this.checkInterval) {
      clearInterval(this.checkInterval);
      this.checkInterval = null;
    }

    this.gainNode.gain.linearRampToValueAtTime(
      0,
      this.context.currentTime + 0.1,
    );
  }

  async resume() {
    console.log('üéµ Resuming audio context');
    if (this.context.state === "suspended") {
      await this.context.resume();
    }
    this.isStreamComplete = false;
    this.scheduledTime = this.context.currentTime + this.initialBufferTime;
    this.gainNode.gain.setValueAtTime(1, this.context.currentTime);
  }

  complete() {
    console.log('üéµ Completing audio stream');
    this.isStreamComplete = true;
    if (this.processingBuffer.length > 0) {
      this.audioQueue.push(this.processingBuffer);
      this.processingBuffer = new Float32Array(0);
      if (this.isPlaying) {
        this.scheduleNextBuffer();
      }
    } else {
      this.onComplete();
    }
  }
}

const GeminiLiveAudio: React.FC<GeminiLiveAudioProps> = ({ 
  apiKey, 
  shouldConnect = false,
  shouldDisconnect = false,
  shouldStartRecording = false,
  onConnectionChange,
  onRecordingStart,
  onMuteChange,
  externalMuted = false,
  onVolumeChange,
  onInVolumeChange,
  onLipsyncUpdate,
  aylaModelRef,
}) => {
  const [isConnected, setIsConnected] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [messages, setMessages] = useState<string[]>([]);
  const [userInput, setUserInput] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [microphoneLevel, setMicrophoneLevel] = useState(0);
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);

  const sessionRef = useRef<Session | undefined>(undefined);
  const responseQueueRef = useRef<LiveServerMessage[]>([]);
  const audioRecorderRef = useRef<SimpleAudioRecorder | undefined>(undefined);
  const audioStreamerRef = useRef<GeminiAudioStreamer | undefined>(undefined);
  const audioContextRef = useRef<AudioContext | undefined>(undefined);
  const initialMessageSentRef = useRef(false);
  
  // Simple transcription storage for lipsync
  const lastTranscriptionTextRef = useRef<string>('');
  const currentWordRef = useRef<{text: string, duration: number} | null>(null);
  
  // Animation queue system to prevent concurrency issues
  const animationQueueRef = useRef<Array<{char: string, duration: number, isNeutral?: boolean}>>([]);
  const isAnimatingRef = useRef<boolean>(false);
  const currentAnimationTimeouts = useRef<NodeJS.Timeout[]>([]);
  const processAnimationQueueRef = useRef<() => void>();
  const addToAnimationQueueRef = useRef<(text: string, duration: number) => void>();

  // Speech recognition for interruption


  // Handle external connection control
  useEffect(() => {
    if (shouldConnect && !isConnected && !isLoading) {
      connectToGemini();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [shouldConnect]);

  // Handle external disconnection control
  useEffect(() => {
    if (shouldDisconnect && isConnected) {
      disconnect();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [shouldDisconnect]);

  // Handle automatic recording start
  useEffect(() => {
    if (shouldStartRecording && isConnected && !isRecording) {
      console.log('üé§ Auto-starting recording after connection...');
      startMicrophoneRecording();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [shouldStartRecording, isConnected, isRecording]);

  // Handle external mute control - simplified
  useEffect(() => {
    // Sync internal mute state
    if (onMuteChange) {
      onMuteChange(externalMuted);
    }
    
    if (externalMuted && isRecording) {
      console.log('üîá External mute: stopping microphone');
      stopMicrophoneRecording();
    } else if (!externalMuted && isConnected && !isRecording) {
      console.log('üé§ External unmute: starting microphone');
      startMicrophoneRecording();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [externalMuted]);

  // Report volume changes to parent
  useEffect(() => {
    if (onVolumeChange) {
      onVolumeChange(0); // For now, we'll use 0. We can enhance this based on audio streamer volume
    }
  }, [onVolumeChange]);

  // Report input volume (microphone level) changes to parent
  useEffect(() => {
    if (onInVolumeChange) {
      onInVolumeChange(microphoneLevel);
    }
  }, [microphoneLevel, onInVolumeChange]);





  // Remove the notification effect that causes loops
  // Keep only connection change notification
  useEffect(() => {
    if (onConnectionChange) {
      onConnectionChange(isConnected);
    }
  }, [isConnected, onConnectionChange]);

  const handleModelTurn = useCallback((message: LiveServerMessage) => {
    console.log('üéØ handleModelTurn called with message:', {
      messageType: message.serverContent?.modelTurn ? 'modelTurn' : 'other',
      hasInlineData: !!message.serverContent?.modelTurn?.parts?.[0]?.inlineData,
      hasText: !!message.serverContent?.modelTurn?.parts?.[0]?.text,
      turnComplete: message.serverContent?.turnComplete,
      fullMessage: message
    });

    if (message.serverContent?.modelTurn?.parts) {
      const part = message.serverContent?.modelTurn?.parts?.[0];

      if (part?.fileData?.fileUri) {
        setMessages(prev => [...prev, `üìÅ File: ${part?.fileData?.fileUri}`]);
      }

      // Audio is already processed in onMessage, just log here
      if (part?.inlineData) {
        const inlineData = part?.inlineData;
        console.log('‚è≠Ô∏è Audio already processed in onMessage:', {
          dataLength: inlineData?.data?.length,
          mimeType: inlineData?.mimeType,
          source: 'handleModelTurn'
        });
      }

      if (part?.text) {
        console.log('üìù AI Response Text (legacy):', part.text);
        console.log(part.text);
        setMessages(prev => [...prev, `ü§ñ Gemini (legacy): ${part?.text}`]);
      }
    }

    // Legacy audio processing (simplified - no duplicate audio)
    if (message.serverContent && !message.serverContent.modelTurn) {
      console.log('üîç Non-modelTurn message:', message.serverContent);
      
      const serverContent = message.serverContent as { inlineData?: { data?: string; mimeType?: string } };
      if (serverContent.inlineData) {
        console.log('‚è≠Ô∏è Legacy audio already processed in onMessage');
      }
    }
  }, []);

  const waitMessage = useCallback(async (): Promise<LiveServerMessage> => {
    let done = false;
    let message: LiveServerMessage | undefined = undefined;
    while (!done) {
      message = responseQueueRef.current.shift();
      if (message) {
        handleModelTurn(message);
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message!;
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const handleTurn = useCallback(async (): Promise<LiveServerMessage[]> => {
    const turn: LiveServerMessage[] = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turn.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turn;
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const connectToGemini = useCallback(async () => {
    if (!apiKey) {
      setError('API key is required');
      console.error('‚ùå API key missing!');
      return;
    }

    setIsLoading(true);
    setError(null);
    console.log('üöÄ Starting Gemini Live connection...');

    try {
      console.log('üîÑ Gemini Live API-y…ô qo≈üuluruq...');
      console.log('API Key uzunluƒüu:', apiKey.length);
      console.log('API Key ilk 10 simvol:', apiKey.substring(0, 10) + '...');

      // Resume audio context first
      if (audioStreamerRef.current) {
        await audioStreamerRef.current.resume();
      }

      const ai = new GoogleGenAI({
        apiKey: apiKey,
      });

      // Your specified model
      // const model = 'models/gemini-2.5-flash-preview-native-audio-dialog';
      const model = 'models/gemini-2.5-flash-exp-native-audio-thinking-dialog';
      
      const greeting = {
        name: "greeting",
        description: "Greeting qrafikini JSON formatda g√∂st…ôr .",
        parameters: {
          type: 'OBJECT',
          properties: {
            json_graph: {
              type: 'STRING',
              description:
                "JSON STRING representation of the graph to render. Must be a string, not a json object",
            },
          },
          required: ["json_graph"],
        },
      } as FunctionDeclaration;

      const altair = {
        name: "altair",
        description: "Altair qrafikini JSON formatda g√∂st…ôr .",
        parameters: {
          type: 'OBJECT',
          properties: {
            json_graph: {
              type: 'STRING',
              description:
                "JSON STRING representation of the graph to render. Must be a string, not a json object",
            },
          },
          required: ["json_graph"],
        },
      } as FunctionDeclaration;

      const config = {
        responseModalities: [Modality.AUDIO],
        mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM,
        speechConfig: {
          language_code: 'az-AZ', // Azerbaijani language
          voiceConfig: {
            prebuiltVoiceConfig: {
              voiceName: 'Aoede',
            }
          }
        },
        outputAudioTranscription: {
          enable: true
        },
        contextWindowCompression: {
          triggerTokens: '25600',
          slidingWindow: { targetTokens: '12800' },
        },
        systemInstruction: {
          text: `# Sorƒüular v…ô R…ôy Toplama Agentinin T…ôlimatƒ±

## Kimlik v…ô M…ôqs…ôd

Sizin adƒ±nƒ±z Ayladƒ±r, Birbankƒ±n r…ôy toplayan virtual assistentisiniz. ∆èsas m…ôqs…ôdiniz maraqlƒ± sorƒüular aparmaq, m√º≈üt…ôri fikirl…ôrini toplamaq v…ô bazar ara≈üdƒ±rma m…ôlumatlarƒ±nƒ± …ôld…ô etm…ôkdir ‚Äì bu zaman is…ô y√ºks…ôk tamamlanma faizini v…ô keyfiyy…ôtli cavablarƒ± t…ômin etm…ôkdir. Sorƒüu zamanƒ± Az…ôrbaycan Beyn…ôlxalq Bankƒ± s√∂z√ºn√º qƒ±sa formada Bank il…ô ifad…ô et.

## S…ôs v…ô ≈û…ôxsiyy…ôt

### ≈û…ôxsiyy…ôt
- Mehriban, dostyana, zarafatcƒ±l, enerjili v…ô diqq…ôtli s…ôsl…ônin
- Maraqlƒ± v…ô diqq…ôtli g√∂r√ºn, amma h…ôdd…ôn artƒ±q co≈üqun olmayƒ±n
- Pe≈ü…ôkar, amma s√∂hb…ôt t…ôrzind…ô danƒ±≈üƒ±q saxlayƒ±n
- Q…ôr…ôzsiz olun v…ô cavablarƒ± y√∂nl…ôndirm…ôyin
- Adƒ±nƒ± t…ôqdim etm…ôsini ist…ôyin v…ô adƒ± il…ô s…ôsl…ônin. 
- ∆èg…ôr s…ôs ki≈üi s…ôsisirs…ô b…ôy, qƒ±z s…ôsisirs…ô xanƒ±m olaraq xitab etm…ôlisiniz.
- ƒ∞lk mesajda adƒ±nƒ± soru≈üun v…ô indi 2 d…ôqiq…ô vaxtƒ±nƒ±z varmƒ±? dey…ô soru≈üun

### Danƒ±≈üƒ±q X√ºsusiyy…ôtl…ôri
- Suallarƒ± aydƒ±n v…ô qƒ±sa dill…ô verin
- Rahat v…ô √∂l√ß√ºl√º danƒ±≈üƒ±q tempi saxlayƒ±n
- Cavablarƒ± t…ôsirl…ôndir…ô bil…ôc…ôk s√∂zl…ôrd…ôn √ß…ôkinin

## Danƒ±≈üƒ±q Axƒ±nƒ±

### Giri≈ü v…ô Razƒ±lƒ±q
Bel…ô ba≈ülayƒ±n: "Xidm…ôtl…ôrimiz bar…ôd…ô qƒ±sa, sorƒüu aparƒ±rƒ±q. Bu biz…ô xidm…ôt keyfiyy…ôtimizin yax≈üƒ±la≈üdƒ±rƒ±lmasƒ±na k√∂m…ôk ed…ôc…ôk. Ba≈ülaya bil…ôrikmi?"

∆èg…ôr t…ôr…ôdd√ºd ed…ôrl…ôrs…ô: "Zamanƒ±nƒ±zƒ±n d…ôy…ôrli olduƒüunu ba≈üa d√º≈ü√ºr…ôm. Sorƒüu qƒ±sa ≈ü…ôkild…ô hazƒ±rlanƒ±b v…ô fikirl…ôriniz birba≈üa olaraq n√∂vb…ôti d…ôf…ô siz…ô daha yax≈üƒ± xidm…ôt etm…ôyimiz…ô t…ôsir ed…ôc…ôk."

### Kontekst Yaratmaq
1. M…ôqs…ôdi izah edin: "Bu sorƒüunun m…ôqs…ôdi [x√ºsusi m…ôqs…ôd]ni anlamaqdƒ±r ki, [t…ô≈ükilat] [respondent…ô v…ô ya ictimaiyy…ôt…ô fayda] …ôld…ô ed…ô bilsin."
2. G√∂zl…ôntil…ôri m√º…ôyy…ôn edin: "Siz…ô [√ºmumi m√∂vzular] √ºzr…ô [sual sayƒ±] sual ver…ôc…ôy…ôm. ∆èks…ôr suallarƒ± cavablandƒ±rmaq yalnƒ±z bir ne√ß…ô saniy…ô √ß…ôk…ôc…ôk."
3. M…ôlumatlarƒ±n gizliliyin…ô …ôminlik yaradƒ±n: "Cavablarƒ±nƒ±z gizli saxlanƒ±lacaq v…ô yalnƒ±z dig…ôr i≈ütirak√ßƒ±larƒ±n r…ôyl…ôri il…ô birlikd…ô t…ôqdim olunacaq."
4. Formatƒ± izah edin: "Sorƒüuya [suallarƒ±n tipi: √ßoxse√ßimli, reytinq miqyasƒ±, a√ßƒ±q suallar] daxildir. D√ºz v…ô ya s…ôhv cavab yoxdur ‚Äì sad…ôc…ô s…ômimi fikirl…ôrinizi bilm…ôk ist…ôyirik."

### Suallarƒ±n Qurulu≈üu v…ô Axƒ±nƒ±
1. Maraq oyadan suallardan ba≈ülayƒ±n:
   - Sad…ô v…ô asan cavab veril…ô bil…ôn suallarla ba≈ülanƒ±r
   - "Son 3 ayda [m…ôhsul/xidm…ôt] istifad…ô etmisiniz?"
   - "Ad…ôt…ôn n…ô q…ôd…ôr tez-tez [m√ºvafiq f…ôaliyy…ôt] edirsiniz?"

2. ∆èsas r…ôy suallarƒ±:
   - M…ômnuniyy…ôt d…ôr…ôc…ôsi: "1-d…ôn 5-…ô q…ôd…ôr olan miqyasda, burada 1 √ßox narazƒ±, 5 is…ô √ßox razƒ±dƒ±r, [x√ºsusi aspekt] il…ô t…ôcr√ºb…ônizi nec…ô qiym…ôtl…ôndir…ôrdiniz?"
   - Konkret t…ôcr√ºb…ôl…ôr: "Son d…ôf…ô [≈üirk…ôt/m…ôhsul] il…ô t…ômasƒ±nƒ±zƒ± n…ôz…ôr…ô alsaq, n…ô x√ºsusil…ô yax≈üƒ± idi?"
   - T…ôkmill…ô≈üdiril…ô bil…ôc…ôk sah…ôl…ôr: "[m…ôhsul/xidm…ôt] hansƒ± c…ôh…ôtl…ôri ehtiyaclarƒ±nƒ±za daha yax≈üƒ± cavab verm…ôk √º√ß√ºn t…ôkmill…ô≈üdiril…ô bil…ôr?"

3. D…ôrinl…ô≈ümi≈ü suallar:
   - X√ºsusi fikird…ôn sonra daha …ôtraflƒ± ara≈üdƒ±rma
   - "Siz [problemi/x√ºsusiyy…ôti] qeyd etdiniz. Bununla baƒülƒ± konkret t…ôcr√ºb…ônizi payla≈üa bil…ôrsinizmi?"
   - "[Qeyd olunan c…ôh…ôt] √ºmumi t…ôcr√ºb…ôniz…ô nec…ô t…ôsir etdi?"

4. K…ômiyy…ôt g√∂st…ôricil…ôri:
   - T√∂vsiy…ô etm…ô ehtimalƒ± (NPS): "0-dan 10-a q…ôd…ôr bir miqyasda, [m…ôhsul/xidm…ôt]i dostunuza v…ô ya h…ômkarƒ±nƒ±za t√∂vsiy…ô etm…ô ehtimalƒ±nƒ±z n…ô q…ôd…ôrdir?"
   - M√ºqayis…ô suallarƒ±: "[Alternativl…ôrl…ô] m√ºqayis…ôd…ô [m…ôhsul/xidm…ôt] daha yax≈üƒ±, daha pis, yoxsa t…ôxmin…ôn eynidir?"
   - G…ôl…ôc…ôk niyy…ôt suallarƒ±: "G…ôl…ôc…ôkd…ô [m…ôhsul/xidm…ôt]d…ôn istifad…ô etm…ô ehtimalƒ±nƒ±z n…ô q…ôd…ôrdir?"

5. Demoqrafik v…ô t…ôsnifatlandƒ±rƒ±cƒ± suallar (ad…ôt…ôn sonda):
   - "Yekun n…ôtic…ôl…ôrin t…ôhlili √º√ß√ºn bir ne√ß…ô t…ôsnifat sualƒ±..."
   - H…ôssas suallarƒ± k√∂n√ºll√º edin: "∆èg…ôr b√∂l√º≈üm…ôkd…ô problem yoxdursa, a≈üaƒüƒ±dakƒ± ya≈ü qruplarƒ±ndan hansƒ±na daxilsiniz?"

### Cavablarƒ±n ƒ∞dar…ô Edilm…ôsi

#### Reytinq miqyasƒ± suallarƒ± √º√ß√ºn
1. Aydƒ±n sual verin: "1-d…ôn 5-…ô q…ôd…ôr olan miqyasda, burada 1 tam razƒ± deyil…ôm, 5 is…ô tam razƒ±yam, '[x√ºsusi ifad…ô]' fikrinizi nec…ô qiym…ôtl…ôndirirsiniz?"
2. Nadir cavablarƒ± t…ôsdiql…ôyin: "Siz bunu [√ßox a≈üaƒüƒ±/y√ºks…ôk reytinq] il…ô qiym…ôtl…ôndirdiniz. Bu reytinq…ô n…ôyin s…ôb…ôb olduƒüunu s√∂yl…ôy…ô bil…ôrsinizmi?"
3. Cavabƒ± t…ôsdiql…ôyin: "[n√∂mr…ô] cavabƒ±nƒ±zƒ± qeyd etdim."

#### A√ßƒ±q suallar √º√ß√ºn
1. Sual verin v…ô d√º≈ü√ºnm…ôy…ô imkan verin: "[m…ôhsul/xidm…ôt]in t…ôkmill…ô≈üdirilm…ôsi √º√ß√ºn hansƒ± t…ôklifl…ôriniz var?" ‚Äì sonra s…ôssizlik saxlayƒ±n
2. Z…ôruri hallarda izah ist…ôyin: "Bu fikri bir az a√ßƒ±qlaya bil…ôrsiniz?" v…ô ya "Bu ba≈ü ver…ôn zaman konkret bir n√ºmun…ô payla≈üa bil…ôrsiniz?"
3. Anlayƒ±≈üƒ± yoxlayƒ±n: "Dem…ôk ist…ôyirsiniz ki, [cavabƒ±n parafrazi]. Doƒürudurmu?"

#### √áoxse√ßimli suallar √º√ß√ºn
1. Se√ßiml…ôri aydƒ±n t…ôqdim edin: "A≈üaƒüƒ±dakƒ± cavablardan hansƒ± t…ôcr√ºb…ônizi …ôn yax≈üƒ± t…ôsvir edir: M√ºk…ômm…ôl, Yax≈üƒ±, Orta v…ô ya Z…ôif?"
2. "Dig…ôr" cavablarƒ± idar…ô edin: "'Dig…ôr' se√ßimini qeyd etdiniz. Xahi≈ü edir…ôm, n…ôyi n…ôz…ôrd…ô tutduƒüunuzu d…ôqiql…ô≈üdirin."
3. Qarƒ±≈üƒ±q cavablarƒ± aydƒ±nla≈üdƒ±rƒ±n: "Sad…ôc…ô d…ôqiql…ô≈üdirm…ôk √º√ß√ºn, [variant A] yoxsa [variant B] se√ßirsiniz?"

### √áox t…ônqidi v…ô ya m…ônfi r…ôy ver…ôn respondentl…ôr √º√ß√ºn
1. A√ßƒ±q ≈ü…ôkild…ô qar≈üƒ±layƒ±n: "T…ôcr√ºb…ôniz bar…ôd…ô a√ßƒ±q danƒ±≈üdƒ±ƒüƒ±nƒ±z √º√ß√ºn t…ô≈ü…ôkk√ºr edir…ôm."
2. M√ºdafi…ô etm…ôyin: M…ônfi r…ôyl…ôri …ôsaslandƒ±rmaq v…ô ya izah etm…ôk olmaz
3. Konstruktiv ≈ü…ôkild…ô ara≈üdƒ±rƒ±n: "Bu t…ôcr√ºb…ôni sizin √º√ß√ºn daha yax≈üƒ± ed…ôn n…ô olardƒ±?"
4. D…ôy…ôrl…ôndirin: "Bel…ô r…ôy x√ºsusil…ô t…ôkmill…ô≈üdirm…ô imkanlarƒ±nƒ± m√º…ôyy…ônl…ô≈üdirm…ôk √º√ß√ºn √ßox d…ôy…ôrlidir."

### Texniki v…ô ya sorƒüu il…ô baƒülƒ± probleml…ôr zamanƒ±
1. Suallarƒ±n aydƒ±n olmamasƒ± zamanƒ±: "N…ôyi soru≈üduƒüumuzu izah edim..."
2. Reytinq miqyasƒ± il…ô baƒülƒ± √ßa≈üqƒ±nlƒ±q zamanƒ±: "Bu sualda 1 [a≈üaƒüƒ± s…ôviyy…ônin izahƒ±], 5 is…ô [y√ºks…ôk s…ôviyy…ônin izahƒ±] dem…ôkdir."
3. Baƒülantƒ± probleml…ôri zamanƒ±: "K…ôsinti √º√ß√ºn √ºzr ist…ôyir…ôm. Son e≈üitdiyim cavab [son aydƒ±n m√∂vzu] haqqƒ±nda idi. Oradan davam ed…ô bil…ôrikmi?"
4. Sorƒüu yorƒüunluƒüu zamanƒ±: "Artƒ±q sorƒüunun [tamamlanmƒ±≈ü hiss…ôsi]% hiss…ôsini ke√ßmi≈üik. T…ôxmin…ôn [qalan vaxt] d…ôqiq…ô qalƒ±b. Davam etm…ôk ist…ôrdiniz, yoxsa sonra z…ông edim?"

## Bilik Bazasƒ±

### Sorƒüu Metodologiyasƒ±
- Q…ôr…ôzsiz sual formala≈üdƒ±rmanƒ±n …ôn yax≈üƒ± √ºsullarƒ±
- Miqyasƒ±n d√ºzg√ºn t…ôqdimatƒ± v…ô izahƒ±
- A√ßƒ±q suallar √º√ß√ºn ara≈üdƒ±rma (probing) texnikalarƒ±
- Cavablarƒ±n doƒüruluƒüunu yoxlama yollarƒ±
- "Bilmir…ôm" v…ô ya "Fikrim yoxdur" cavablarƒ±nƒ± idar…ôetm…ô

### Sorƒüu M…ôzmunu
- Sual m…ôtni v…ô t…ôsdiql…ônmi≈ü variantlarƒ±
- Baƒülƒ± suallar √º√ß√ºn cavab se√ßiml…ôri
- Ke√ßid loqikasƒ± v…ô ≈ü…ôrti suallar
- Qarƒ±≈üƒ±q suallar √º√ß√ºn icaz…ô verilmi≈ü aydƒ±nla≈üdƒ±rmalar
- Demoqrafik t…ôsnifat kateqoriyalarƒ±

### Xidm…ôt/M…ôhsul M…ôlumatƒ±
- Ara≈üdƒ±rƒ±lan m…ôhsul/xidm…ôtl…ôr haqqƒ±nda …ôsas anlayƒ±≈ülar
- Sah…ôy…ô aid √ºmumi terminologiya
- Son d…ôyi≈üiklikl…ôr v…ô probleml…ôr bar…ôd…ô m…ôlumatlƒ±lƒ±q
- R…ôqibl…ôr v…ô bazar konteksti
- ∆èvv…ôlki ara≈üdƒ±rma n…ôtic…ôl…ôri v…ô trend n√ºmun…ôl…ôri

### M…ôlumat Keyfiyy…ôti Standartlarƒ±
- Etibarlƒ± cavab meyarlarƒ±
- Tam sayƒ±lan sorƒüular √º√ß√ºn minimum t…ôl…ôbl…ôr
- Lazƒ±m g…ôl…ôrs…ô, demoqrafik kvotalar
- S…ôthi v…ô qeyri-s…ômimi cavab …ôlam…ôtl…ôri
- Cavablarƒ±n doƒüruluƒüunu yoxlama √ºsullarƒ±

## Cavablarƒ±n Yekun Emalƒ±

- Reytinq miqyasƒ±nƒ± t…ôqdim ed…ôrk…ôn bel…ô deyin: "N√∂vb…ôti bir ne√ß…ô sualda m√ºxt…ôlif c…ôh…ôtl…ôri 1-d…ôn 5-…ô q…ôd…ôr qiym…ôtl…ôndirm…ôyinizi xahi≈ü ed…ôc…ôy…ôm. Burada 1 [az], 5 is…ô [√ßox] dem…ôkdir."
- M√∂vzu d…ôyi≈ü…ôrk…ôn ke√ßid c√ºml…ôsi: "ƒ∞ndi t…ôcr√ºb…ônizin ba≈üqa bir t…ôr…ôfi bar…ôd…ô soru≈ümaq ist…ôyir…ôm: [yeni m√∂vzu]."
- Maraqlƒ± cavablardan sonra davam edin: "Bu maraqlƒ± bir fikirdir. Bu n…ôtic…ôy…ô g…ôlm…ôyiniz…ô s…ôb…ôb n…ô olub, a√ßƒ±qlaya bil…ôrsinizmi?"
- ∆ètraflƒ± cavablarƒ± t…ô≈üviq etm…ôk √º√ß√ºn: "Bu bar…ôd…ô d√º≈ü√ºnc…ônizi biziml…ô b√∂l√º≈ü…ô bil…ôrsinizmi?" v…ô ya "Bu fikr…ô g…ôlm…ôyiniz…ô t…ôsir ed…ôn amill…ôr n…ôl…ôrdir?"

## Z…ôngl…ôrin ƒ∞dar…ôedilm…ôsi

- ∆èg…ôr respondent…ô a√ßƒ±qlama lazƒ±mdƒ±rsa: "M…ômnuniyy…ôtl…ô izah ed…ôr…ôm. Bu sual [aydƒ±n t…ôkrar] bar…ôsind…ôdir v…ô [m…ôqs…ôd]i anlamaƒüa y√∂n…ôlib."
- Respondent diqq…ôtini itiribs…ô: "Ba≈üqa i≈ül…ôriniz ola bil…ôr, bunu anlayƒ±ram. Sorƒüunu davam etdirm…ôk ist…ôrdiniz, yoxsa daha m√ºnasib vaxtda z…ông edim?"
- Sualƒ± t…ôkrar verm…ôk lazƒ±m olduqda: "Sualƒ±n aydƒ±n olmasƒ± √º√ß√ºn bir daha t…ôkrarlayƒ±m: [sualƒ±n t…ôkrarƒ±]."
- Texniki √ß…ôtinlikl…ôr ba≈ü ver…ôrs…ô: "Texniki nasazlƒ±ƒüa g√∂r…ô √ºzr ist…ôyir…ôm. Cavabƒ±nƒ±zƒ± qeyd etdim."

Unutmayƒ±n ki, …ôsas m…ôqs…ôd respondentl…ôrin fikirl…ôrini v…ô t…ôcr√ºb…ôl…ôrini d√ºzg√ºn v…ô q…ôr…ôzsiz ≈ü…ôkild…ô toplamaqdƒ±r. M…ôlumatƒ±n keyfiyy…ôti sizin birinci prioritetinizdir, i≈ütirak√ßƒ± √º√ß√ºn is…ô m√ºsb…ôt v…ô h√∂rm…ôtli t…ôcr√ºb…ô t…ômin etm…ôk ikinci prioritetdir.`,
        },
        tools: [
          // there is a free-tier quota for search
          { googleSearch: {} },
          { functionDeclarations: [greeting,altair] },
        ],
      };

      console.log('üìã Model:', model);
      console.log('üìã Konfigur–∞—Åiya:', JSON.stringify(config, null, 2));

      setMessages(prev => [...prev, `üîÑ ${model} modelin…ô qo≈üuluruq...`]);

      const session = await ai.live.connect({
        model,
        callbacks: {
          onopen: function () {
            console.log('‚úÖ Live session opened successfully');
            setIsConnected(true);
            setIsLoading(false);
            setMessages(prev => [...prev, '‚úÖ Gemini Live Audio-ya uƒüurla qo≈üuldu']);
          },
          onmessage: function (message: LiveServerMessage) {
            // Handle interruption - check first before processing other content
            if (message.serverContent?.interrupted) {
              console.log('‚ö° Audio interrupted by user speech');
              if (audioStreamerRef.current) {
                audioStreamerRef.current.stop();
              }
              setIsAudioPlaying(false);
              setMessages(prev => [...prev, '‚ö° Interruption - ƒ∞stifad…ô√ßi danƒ±≈üdƒ±']);
              
              // Reset character animation to neutral
              if (aylaModelRef?.current) {
                const neutralTargets = getPhonemeTargets('neutral');
                aylaModelRef.current.updateMorphTargets(neutralTargets);
              }
              
              // Clear animation queue
              animationQueueRef.current = [];
              isAnimatingRef.current = false;
              currentAnimationTimeouts.current.forEach(timeout => clearTimeout(timeout));
              currentAnimationTimeouts.current = [];
              
              return; // Don't process other content when interrupted
            }
            
            // Handle audio transcription messages  
            if (message.serverContent?.outputTranscription) {
              const transcription = message.serverContent.outputTranscription;
              // console.log('üé§ AI Audio Transcription:', transcription);
              setMessages(prev => [...prev, `üéôÔ∏è AI S…ôsi (transcribe): ${transcription}`]);
              
              // Store latest transcription text for lipsync
              if (typeof transcription === 'object' && transcription.text) {
                lastTranscriptionTextRef.current = transcription.text;
                // console.log('üìù Latest transcription stored:', transcription.text);
              } else if (typeof transcription === 'string') {
                lastTranscriptionTextRef.current = transcription;
                // console.log('üìù Latest transcription stored:', transcription);
              }
            }
            
            // Process audio immediately when it arrives
            if (message.serverContent?.modelTurn?.parts) {
              const part = message.serverContent?.modelTurn?.parts?.[0];

              if (part?.inlineData?.data && audioStreamerRef.current) {
                const inlineData = part.inlineData;
                const audioData = inlineData.data!; // We already checked it exists
                
                try {
                  setMessages(prev => [...prev, 'üéµ Audio response alƒ±ndƒ±...']);
                  
                  // Use part text or latest transcription
                  const textForLipsync = part.text || lastTranscriptionTextRef.current;
                  
                  // Calculate duration from base64 audio data
                  const binaryString = atob(audioData);
                  const bytes = new Uint8Array(binaryString.length);
                  for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                  }
                  const sampleCount = bytes.length / 2; // PCM16 = 2 bytes per sample
                  const duration = sampleCount / 24000; // 24kHz sample rate
                  const durationMs = Math.round(duration * 1000); // Convert to milliseconds
                  
                  // Smart duration accumulation logic
                  if (textForLipsync && textForLipsync.trim().length > 0) {
                    // New word/text arrived
                    if (currentWordRef.current) {
                      if (audioStreamerRef.current) {
                        audioStreamerRef.current.onLipsyncUpdate(currentWordRef.current.text, currentWordRef.current.duration);
                      }
                    }
                    
                    // Start new word (silently)
                    currentWordRef.current = {
                      text: textForLipsync.trim(),
                      duration: durationMs
                    };
                  } else {
                    // Empty text - add duration to current word (silently)
                    if (currentWordRef.current) {
                      currentWordRef.current.duration += durationMs;
                    }
                  }
                  
                  // Play audio (simplified - no text parameter needed)
                  audioStreamerRef.current.addBase64Audio(
                    audioData, 
                    inlineData.mimeType ?? 'audio/pcm;rate=24000'
                  );
                  
                  // Clear used transcription
                  if (textForLipsync === lastTranscriptionTextRef.current) {
                    lastTranscriptionTextRef.current = '';
                  }
                } catch (error) {
                  console.error('‚ùå Error processing audio:', error);
                  setMessages(prev => [...prev, '‚ùå Audio prosess x…ôtasƒ±: ' + (error instanceof Error ? error.message : 'Nam…ôlum')]);
                }
              }

              if (part?.text) {
                console.log('üìù AI Text:', part.text);
                setMessages(prev => [...prev, `ü§ñ Gemini: ${part.text}`]);
              }
            }
            
            // Legacy queue for handleTurn (but audio won't be reprocessed)
            responseQueueRef.current.push(message);
          },
          onerror: function (e: ErrorEvent) {
            console.error('‚ùå Live session error:', e);
            setError(`Live session x…ôtasƒ±: ${e.message}`);
            setIsConnected(false);
            setIsLoading(false);
          },
          onclose: function (e: CloseEvent) {
            console.error('üîå Live session closed:', e);
            setIsConnected(false);
            setIsLoading(false);
            const reason = e.reason || `Kod: ${e.code}`;
            setMessages(prev => [...prev, `üîå Live session baƒülandƒ±: ${reason}`]);
          },
        },
        config
      });

      console.log('‚úÖ Live session yaradƒ±ldƒ±:', session);
      sessionRef.current = session;
      
    } catch (error) {
      console.error('‚ùå Live connection failed:', error);
      setIsLoading(false);
      if (error instanceof Error) {
        console.error('Error details:', {
          name: error.name,
          message: error.message,
          stack: error.stack
        });
        
        if (error.message.includes('API key')) {
          setError('API key problemi - key-i yoxlayƒ±n');
        } else if (error.message.includes('permission')) {
          setError('ƒ∞caz…ô problemi - Live API giri≈üi yoxdur');
        } else if (error.message.includes('model')) {
          setError('Model problemi - model m√∂vcud deyil v…ô ya Live API d…ôst…ôkl…ômir');
        } else {
          setError(`Live qo≈üulma x…ôtasƒ±: ${error.message}`);
        }
      } else {
        setError('Nam…ôlum Live qo≈üulma x…ôtasƒ±');
      }
    }
  }, [apiKey]);

  const sendMessage = useCallback(async () => {
    if (!sessionRef.current || !userInput.trim()) {
      return;
    }

    const messageText = userInput.trim();
    setUserInput('');
    setMessages(prev => [...prev, `üë§ Siz: ${messageText}`]);
    
    try {
      console.log('üì§ Sending message:', messageText);
      
      sessionRef.current.sendClientContent({
        turns: [messageText]
      });

      console.log('‚úÖ Message sent, waiting for response...');
      await handleTurn();
    } catch (error) {
      console.error('‚ùå Error sending message:', error);
      setError(`Mesaj g√∂nd…ôrm…ô x…ôtasƒ±: ${error instanceof Error ? error.message : 'Nam…ôlum x…ôta'}`);
    }
  }, [userInput, handleTurn]);

  const startMicrophoneRecording = useCallback(async () => {
    if (!sessionRef.current) {
      setError('∆èvv…ôlc…ô Gemini Live-a qo≈üulun');
      return;
    }

    try {
      if (!audioRecorderRef.current) {
        audioRecorderRef.current = new SimpleAudioRecorder(16000);
        
        // Use EventEmitter pattern
        audioRecorderRef.current.on('data', (base64Data: string) => {
          if (sessionRef.current) {
            sessionRef.current.sendRealtimeInput({
              audio: {
                data: base64Data,
                mimeType: 'audio/pcm;rate=16000'
              }
            });
          }
        });
        
        // Throttle volume updates to prevent excessive state changes
        let lastVolumeUpdate = 0;
        audioRecorderRef.current.on('volume', (volume: number) => {
          const now = Date.now();
          if (now - lastVolumeUpdate > 200) { // Update every 200ms max
          setMicrophoneLevel(volume);
            lastVolumeUpdate = now;
          }
        });
      }

      await audioRecorderRef.current.start();
      setIsRecording(true);
      setMessages(prev => [...prev, 'üé§ Mikrofon ba≈üladƒ±, danƒ±≈üa bil…ôrsiniz...']);
      
      if (onRecordingStart) {
        onRecordingStart();
      }
    } catch (error) {
      console.error('Error starting microphone:', error);
      setError(`Mikrofon x…ôtasƒ±: ${error instanceof Error ? error.message : 'Nam…ôlum x…ôta'}`);
    }
  }, [onRecordingStart]);

  const stopMicrophoneRecording = useCallback(() => {
    if (audioRecorderRef.current) {
      audioRecorderRef.current.stop();
      audioRecorderRef.current = undefined;
    }
    
    setIsRecording(false);
    setMicrophoneLevel(0);
    setMessages(prev => [...prev, 'üîá Mikrofon dayandƒ±rƒ±ldƒ±']);
    
    // No need to trigger handleTurn since we have real-time processing
    console.log('üîá Microphone stopped - real-time processing active');
  }, []);

  const clearMessages = useCallback(() => {
    setMessages([]);
  }, []);

  const disconnect = useCallback(() => {
    if (sessionRef.current) {
      sessionRef.current.close();
      sessionRef.current = undefined;
    }
    
    if (audioRecorderRef.current) {
      audioRecorderRef.current.stop();
      audioRecorderRef.current = undefined;
    }
    
    if (audioStreamerRef.current) {
      audioStreamerRef.current.stop();
    }
    
    // Clear animation queue and timeouts
    animationQueueRef.current = [];
    isAnimatingRef.current = false;
    currentAnimationTimeouts.current.forEach(timeout => clearTimeout(timeout));
    currentAnimationTimeouts.current = [];
    
    // Reset character to neutral state
    if (aylaModelRef?.current) {
      const neutralTargets = getPhonemeTargets('neutral');
      aylaModelRef.current.updateMorphTargets(neutralTargets);
    }
    
    setIsConnected(false);
    setIsRecording(false);
    setMicrophoneLevel(0);
    setIsAudioPlaying(false);
    responseQueueRef.current = [];
    
    // Clear accumulated text for lipsync
    lastTranscriptionTextRef.current = '';
    
    // Clear and send final word if exists
    if (currentWordRef.current) {
      // Use addToAnimationQueue for final word
      addToAnimationQueueRef.current?.(currentWordRef.current.text, currentWordRef.current.duration);
      currentWordRef.current = null;
    }
    
    // Reset the initial message flag
    initialMessageSentRef.current = false;
  }, [aylaModelRef]);

  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  // When connection is established, send the initial greeting prompt
  useEffect(() => {
    if (isConnected && sessionRef.current && !initialMessageSentRef.current) {
      console.log('üöÄ Triggering initial AI greeting...');
      sessionRef.current.sendClientContent({
        turns: [{ text: "Salamlƒ±yaraq sorƒüuya ba≈ülayƒ±n." }]
      });
      initialMessageSentRef.current = true;
    }
  }, [isConnected]);

  // Initialize audio streamer with AudioContext
  useEffect(() => {
    const initAudio = async () => {
      try {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const AudioContextClass = (window as any).AudioContext || (window as any).webkitAudioContext;
        const audioContext = new AudioContextClass();
        audioContextRef.current = audioContext;
        audioStreamerRef.current = new GeminiAudioStreamer(audioContext);
        
        audioStreamerRef.current.onAudioStart = (startTime) => {
          console.log('üéµ Audio playback started at:', startTime);
          setIsAudioPlaying(true);
          setMessages(prev => [...prev, 'üîä AI danƒ±≈üƒ±r...']);
        };
        
        audioStreamerRef.current.onAudioProgress = (currentTime, isPlaying) => {
          setIsAudioPlaying(isPlaying);
        };
        
        audioStreamerRef.current.onComplete = () => {
          console.log('üéµ Audio playback completed');
          setIsAudioPlaying(false);
          setMessages(prev => [...prev, 'üîá AI danƒ±≈üƒ±ƒüƒ±nƒ± tamamladƒ±']);
          
          // Send final accumulated word if exists
          if (currentWordRef.current) {
            // Use addToAnimationQueue for final word
            addToAnimationQueueRef.current?.(currentWordRef.current.text, currentWordRef.current.duration);
            currentWordRef.current = null;
          }
        };

        audioStreamerRef.current.onLipsyncUpdate = (text, duration) => {
          //console.log('üéôÔ∏è Lipsync Update - Adding to queue:', { text, duration });
          if (onLipsyncUpdate) {
            onLipsyncUpdate(text, duration);
          }
          
          // Use addToAnimationQueue to break word into characters
          addToAnimationQueueRef.current?.(text, duration);
        };

        console.log('üéµ Audio system initialized');
      } catch (error) {
        console.error('Audio initialization failed:', error);
        setError('Audio sistemi ba≈ülatƒ±la bilm…ôdi');
      }
    };

    initAudio();

    return () => {
      if (audioStreamerRef.current) {
        audioStreamerRef.current.stop();
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, [onLipsyncUpdate]);

  const sanitizeText = (text: string): string[] => {
    // Az…ôrbaycan h…ôrfl…ôri + r…ôq…ôml…ôr + space saxla, qalanƒ±nƒ± evez et _ bununla 
    const cleanText = text.toLowerCase()
      .replace(/[^abc√ßde…ôfgƒühxƒ±ijkqlmno√∂prs≈üt√ºuvyz0-9\s]/g, '_');
    
    // Consecutive consonants to optimize
    const consonantsSecondToRemove = ['r', 'n', 's', 't', 'd', 'k', 'g', 'y', '√ß', 'z', '≈ü', 'q', 'x', 'j', 'h', 'ƒü', 'c', 'l'];
    
    // Split into characters and remove consecutive consonants
    const chars = cleanText.split('').filter(char => char !== ' '); // Remove spaces first
    const optimizedChars: string[] = [];
    
    for (let i = 0; i < chars.length; i++) {
      const currentChar = chars[i];
      const nextChar = chars[i + 1];
      
      // Add current character
      optimizedChars.push(currentChar);
      
      // If current and next are both in consonants list, skip the next one
      if (nextChar && 
          consonantsSecondToRemove.includes(currentChar) && 
          consonantsSecondToRemove.includes(nextChar)) {
        console.log(`üîÑ Removing consecutive consonant: ${currentChar}${nextChar} -> ${currentChar}`);
        i++; // Skip next character
      }
    }
    
    console.log(`üìù Original: "${text}" (${chars.length} chars) -> Optimized: "${optimizedChars.join('')}" (${optimizedChars.length} chars)`);
    
    return optimizedChars;
  };

  // Add word to animation queue by breaking it into characters
  const addToAnimationQueue = useCallback((text: string, totalDuration: number) => {
    console.log('üìù Adding word to queue:', { text, totalDuration });

    // Check for greeting keywords
    const greetingKeywords = ['salam', 'hello', 'hi', 'hey', 'greetings'];
    const lowerCaseText = text.toLowerCase();
    if (greetingKeywords.some(keyword => lowerCaseText.includes(keyword))) {
      console.log('üëã Greeting detected in queue! Playing animation...');
      aylaModelRef?.current?.playGreetingAnimation();
    }
    
    const chars = sanitizeText(text);
    if (chars.length === 0) return;
    
    const durationPerChar = Math.min(150, Math.round(totalDuration / chars.length));
    
    // Add each character to queue with its duration
    chars.forEach((char, index) => {
      animationQueueRef.current.push({
        char: char,
        duration: durationPerChar,
        isNeutral: false
      });
    });
    
    console.log('üìù Queue length after adding word:', animationQueueRef.current.length);
    
    // Queue listener will automatically pick up the new items
  }, []);

  // Animation queue processor to handle sequential character animations
  const processAnimationQueue = useCallback(() => {
    if (isAnimatingRef.current || animationQueueRef.current.length === 0) {
      return;
    }
    
    isAnimatingRef.current = true;
    const nextAnimation = animationQueueRef.current.shift()!;
    const { char, duration, isNeutral } = nextAnimation;
    
    console.log('üé¨ Processing character:', { char, duration, isNeutral });
    
    // Clear any existing timeouts
    currentAnimationTimeouts.current.forEach(timeout => clearTimeout(timeout));
    currentAnimationTimeouts.current = [];
    
    // Apply morph targets for this character
    const morphTargets = getPhonemeTargets(char);
    aylaModelRef?.current?.updateMorphTargets(morphTargets);
    
    // Schedule next character processing after this duration
    const timeout = setTimeout(() => {
      isAnimatingRef.current = false;
      
      // The queue listener will automatically process the next character
      console.log('üé¨ Character animation completed, ready for next');
    }, duration);
    
    currentAnimationTimeouts.current.push(timeout);
  }, []);
  
  // Assign the functions to refs in useEffect
  useEffect(() => {
    processAnimationQueueRef.current = processAnimationQueue;
    addToAnimationQueueRef.current = addToAnimationQueue;
  }, [processAnimationQueue, addToAnimationQueue]);

  // Real-time queue listener - monitors queue constantly
  useEffect(() => {
    const queueListener = setInterval(() => {
      // If there are items in queue and we're not currently animating, start processing
      if (animationQueueRef.current.length > 0 && !isAnimatingRef.current) {
        console.log('üéØ Queue listener detected items, starting processing...');
        processAnimationQueueRef.current?.();
      }
    }, 50); // Check every 50ms for real-time responsiveness

    return () => {
      clearInterval(queueListener);
    };
  }, []);

const getPhonemeTargets = (phoneme: string): MorphTargetData[] => {
    if (!phoneme) return [{ morphTarget: "Merged_Open_Mouth", weight: "0" }];
    switch (phoneme) {
        case 'a': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.4" }];
        case '…ô': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.5" }];
        case 'i': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }, { morphTarget: "V_Wide", weight: "0.5" }];
        case 'l': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'r': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'n': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'm': return [{ morphTarget: "V_Explosive", weight: "1" }];
        case 'e': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.3" }, { morphTarget: "V_Wide", weight: "0.4" }];
        case 's': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 't': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'd': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'k': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'b': return [{ morphTarget: "V_Explosive", weight: "1" }];
        case 'g': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'y': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'u': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.1" }, { morphTarget: "V_Affricate", weight: "1" }, { morphTarget: "V_Tight", weight: "1" }];
        case 'o': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }, { morphTarget: "V_Affricate", weight: "1" }, { morphTarget: "V_Tight", weight: "1" }];
        case '√ß': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'z': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case '≈ü': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'q': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'x': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'v': return [{ morphTarget: "V_Dental_Lip", weight: "1" }];
        case 'j': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case '√º': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.1" }, { morphTarget: "V_Affricate", weight: "1" }, { morphTarget: "V_Tight", weight: "1" }];
        case '√∂': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }, { morphTarget: "V_Affricate", weight: "1" }, { morphTarget: "V_Tight", weight: "1" }];
        case 'h': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
        case 'ƒü': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'c': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }]; 
        case 'ƒ±': return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }, { morphTarget: "V_Wide", weight: "0.6" }];
        case 'p': return [{ morphTarget: "V_Explosive", weight: "1" }];
        case 'f': return [{ morphTarget: "V_Dental_Lip", weight: "1" }];
        case '_': case 'neutral': return [
            { morphTarget: "Merged_Open_Mouth", weight: "0" }, 
            { morphTarget: "V_Lip_Open", weight: "0" }, 
            { morphTarget: "V_Tight_O", weight: "0" }, 
            { morphTarget: "V_Dental_Lip", weight: "0" }, 
            { morphTarget: "V_Explosive", weight: "0" }, 
            { morphTarget: "V_Wide", weight: "0" }, 
            { morphTarget: "V_Affricate", weight: "0" },
            { morphTarget: "V_Tight", weight: "0" }
        ];
        default: return [{ morphTarget: "Merged_Open_Mouth", weight: "0.2" }];
    }
};

  return (
    <div className="gemini-live-audio">
      <div className="header">
        <h2>üé§ Gemini Live Voice Chat</h2>
        <div className="connection-status">
          <span className={`status-indicator ${isConnected ? 'connected' : 'disconnected'}`}>
            {isConnected ? 'üü¢ Live Qo≈üuldu' : 'üî¥ Live Qo≈üulmayƒ±b'}
          </span>
        </div>
      </div>

      {error && (
        <div className="error-message">
          ‚ùå {error}
        </div>
      )}

      <div className="controls">
        {!isConnected ? (
          <button 
            onClick={connectToGemini} 
            disabled={isLoading}
            className="connect-btn"
          >
            {isLoading ? 'üîÑ Live Qo≈üulur...' : 'üîó Live Voice Chat Ba≈ülat'}
          </button>
        ) : (
          <button 
            onClick={disconnect}
            className="disconnect-btn"
          >
            üîå Live Ayƒ±r
          </button>
        )}

        {isConnected && (
          <>
            {!isRecording ? (
              <button 
                onClick={startMicrophoneRecording}
                className="mic-btn"
              >
                üé§ Voice Chat Ba≈ülat
              </button>
            ) : (
              <button 
                onClick={stopMicrophoneRecording}
                className="mic-stop-btn"
              >
                üîá Voice Chat Dayandƒ±r
              </button>
            )}
          </>
        )}
        
        <button 
          onClick={clearMessages}
          className="clear-btn"
        >
          üóëÔ∏è T…ômizl…ô
        </button>
      </div>

      {/* Voice activity indicators */}
      {(isRecording || isAudioPlaying) && (
        <div className="audio-status">
          {isRecording && (
            <div className="mic-level">
              <span>üé§ Danƒ±≈üƒ±n:</span>
              <div className="level-bar">
                <div 
                  className="level-fill" 
                  style={{ width: `${microphoneLevel * 100}%` }}
                />
              </div>
            </div>
          )}
          
          {isAudioPlaying && (
            <div className="audio-playing">
              üîä AI danƒ±≈üƒ±r...
            </div>
          )}
        </div>
      )}

      {isConnected && (
        <div className="message-input">
          <input
            type="text"
            value={userInput}
            onChange={(e) => setUserInput(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
            placeholder="Gemini-y…ô mesajƒ±nƒ±zƒ± yazƒ±n (v…ô ya voice chat istifad…ô edin)..."
            className="input-field"
          />
          <button 
            onClick={sendMessage}
            disabled={!userInput.trim()}
            className="send-btn"
          >
            üì§ Text G√∂nd…ôr
          </button>
        </div>
      )}

      <div className="messages-container">
        <h3>üí¨ Live S√∂hb…ôt</h3>
        <div className="messages">
          {messages.length === 0 ? (
            <p className="no-messages">Live voice chat ba≈ülatƒ±n v…ô danƒ±≈ümaƒüa ba≈ülayƒ±n!</p>
          ) : (
            messages.map((message, index) => (
              <div key={index} className="message">
                {message}
              </div>
            ))
          )}
        </div>
      </div>
    </div>
  );
};

export default GeminiLiveAudio; 